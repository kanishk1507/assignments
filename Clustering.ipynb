{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMsHzJCzQFxB1VU6tbY+Tv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thepersonuadmire/Clustering/blob/main/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theorectical Questions"
      ],
      "metadata": {
        "id": "hhulVIs7OhrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is unsupervised learning in the context of machine learning?\n"
      ],
      "metadata": {
        "id": "iFndbQT7O2Co"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised learning is a type of machine learning where the model is trained on data without labeled responses. The goal is to identify patterns, structures, or relationships within the data. Common tasks include clustering, dimensionality reduction, and anomaly detection."
      ],
      "metadata": {
        "id": "g5dNf3gXXofc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does K-Means clustering algorithm work?\n"
      ],
      "metadata": {
        "id": "_JuSRoUUO6fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means clustering works by partitioning the dataset into K distinct clusters. The algorithm follows these steps:\n",
        "\n",
        "Initialize K centroids randomly.\n",
        "\n",
        "Assign each data point to the nearest centroid.\n",
        "\n",
        "Update the centroids by calculating the mean of all points assigned to each cluster.\n",
        "\n",
        "Repeat the assignment and update steps until convergence (i.e., when assignments no longer change)."
      ],
      "metadata": {
        "id": "zvYgmiPBXqUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of a dendrogram in hierarchical clustering?\n"
      ],
      "metadata": {
        "id": "74xH7oaSO8vL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dendrogram is a tree-like diagram that illustrates the arrangement of clusters formed by hierarchical clustering. It shows the relationships between clusters and how they are merged at various levels of similarity. The height of the branches indicates the distance or dissimilarity between clusters."
      ],
      "metadata": {
        "id": "JQR7vi_QXv2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the main difference between K-Means and Hierarchical Clustering?\n"
      ],
      "metadata": {
        "id": "iVaEkdJBO-nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference is that K-Means is a partitional clustering method that requires the number of clusters (K) to be specified in advance, while hierarchical clustering builds a hierarchy of clusters without needing to predefine the number of clusters. Hierarchical clustering can be agglomerative (bottom-up) or divisive (top-down).\n",
        "\n"
      ],
      "metadata": {
        "id": "-YlSV4XEXzB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are the advantages of DBSCAN over K-Means?\n"
      ],
      "metadata": {
        "id": "Mo4wz1QMPA7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " DBSCAN (Density-Based Spatial Clustering of Applications with Noise) has several advantages:\n",
        "\n",
        "It can find arbitrarily shaped clusters, while K-Means assumes spherical clusters.\n",
        "\n",
        "It can identify noise points as outliers, which K-Means cannot do.\n",
        "\n",
        "It does not require the number of clusters to be specified in advance."
      ],
      "metadata": {
        "id": "LJB6Vc79X2Bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. When would you use Silhouette Score in clustering?\n"
      ],
      "metadata": {
        "id": "MXrMOW91PCwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Silhouette Score is used to evaluate the quality of clustering. It measures how similar an object is to its own cluster compared to other clusters. A higher Silhouette Score indicates better-defined clusters. It is particularly useful for determining the optimal number of clusters."
      ],
      "metadata": {
        "id": "mu-M7UCHX7et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What are the limitations of Hierarchical Clustering?\n"
      ],
      "metadata": {
        "id": "wwqJun8nPEfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Limitations of hierarchical clustering include:\n",
        "\n",
        "It can be computationally expensive, especially for large datasets (O(n^3) complexity).\n",
        "\n",
        "It is sensitive to noise and outliers.\n",
        "\n",
        "Once a merge or split is made, it cannot be undone, which can lead to suboptimal clustering."
      ],
      "metadata": {
        "id": "J27tt5JxX-vG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Why is feature scaling important in clustering algorithms like K-Means?\n"
      ],
      "metadata": {
        "id": "c6UOl0nLPG5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is important because K-Means uses distance metrics (like Euclidean distance) to assign points to clusters. If features are on different scales, those with larger ranges can disproportionately influence the clustering results. Scaling ensures that all features contribute equally to the distance calculations."
      ],
      "metadata": {
        "id": "OYRXnR80YDgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does DBSCAN identify noise points?\n"
      ],
      "metadata": {
        "id": "wN2CzabyPK5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN identifies noise points as those points that do not belong to any cluster. A point is considered noise if it is not within the ε-neighborhood of any core point (a point with a minimum number of neighbors within ε). Noise points are those that are neither core points nor directly reachable from core points."
      ],
      "metadata": {
        "id": "lrqQcD3fYGif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Define inertia in the context of K-Means?\n"
      ],
      "metadata": {
        "id": "KUU0pcsNPNCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nertia is a measure of how tightly the clusters are packed. It is defined as the sum of squared distances between each point and its assigned cluster centroid. Lower inertia values indicate better clustering, as they suggest that points are closer to their centroids."
      ],
      "metadata": {
        "id": "sSP9n79yYKx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the elbow method in K-Means clustering?\n"
      ],
      "metadata": {
        "id": "M1UEVXIdPPCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The elbow method is a technique used to determine the optimal number of clusters (K) in K-Means clustering. It involves plotting the inertia against the number of clusters and looking for a \"knee\" or \"elbow\" point where the rate of decrease sharply changes. This point suggests a suitable number of clusters."
      ],
      "metadata": {
        "id": "4sfyb3biYNv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Describe the concept of \"density\" in DBSCAN.\n"
      ],
      "metadata": {
        "id": "szOLDY0uPQ7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In DBSCAN, density refers to the number of points within a specified radius (ε) around a point. A point is considered a core point if it has at least a minimum number of neighbors (MinPts) within this radius. Clusters are formed from core points and their reachable points, while low-density areas are classified as noise."
      ],
      "metadata": {
        "id": "9QFMPDE8YQfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Can hierarchical clustering be used on categorical data?\n"
      ],
      "metadata": {
        "id": "SVm94Fi3PSwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Yes, hierarchical clustering can be used on categorical data, but it requires appropriate distance metrics (e.g., Hamming distance or Jaccard distance) that are suitable for categorical variables. Standard distance measures like Euclidean distance are not appropriate for categorical data."
      ],
      "metadata": {
        "id": "tXsG-N-eYTi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What does a negative Silhouette Score indicate?\n"
      ],
      "metadata": {
        "id": "mQScU2jUPUXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A negative Silhouette Score indicates that a data point is likely assigned to the wrong cluster. It suggests that the point is closer to points in other clusters than to points in its own cluster, indicating poor clustering quality."
      ],
      "metadata": {
        "id": "08x4QbhEYXy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Explain the term \"linkage criteria\" in hierarchical clustering.\n"
      ],
      "metadata": {
        "id": "K7aMEMNgPWGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linkage criteria determine how the distance between clusters is calculated during the merging process in hierarchical clustering. Common linkage methods include:\n",
        "\n",
        "Single linkage: distance between the closest points of two clusters.\n",
        "\n",
        "Complete linkage: distance between the farthest points of two clusters.\n",
        "\n",
        "Average linkage: average distance between all pairs of points in two clusters."
      ],
      "metadata": {
        "id": "hPlsnQifYaya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n"
      ],
      "metadata": {
        "id": "cZpWc00jPXw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means assumes that clusters are spherical and of similar size. If clusters have varying sizes or densities, K-Means may struggle to accurately assign points to clusters, leading to poor clustering results. It may merge smaller clusters into larger ones or fail to capture the true structure of the data."
      ],
      "metadata": {
        "id": "8tSvuqbVYf7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the core parameters in DBSCAN, and how do they influence clustering?\n"
      ],
      "metadata": {
        "id": "ZQ9UTR82PZaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core parameters in DBSCAN are:\n",
        "\n",
        "ε (epsilon): the radius within which to search for neighbors. A larger ε can merge clusters, while a smaller ε may result in more noise points.\n",
        "\n",
        "MinPts: the minimum number of points required to form a dense region. A higher MinPts value can lead to fewer clusters and more noise, while a lower value may result in more clusters.\n"
      ],
      "metadata": {
        "id": "6npNOYHWYiwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. How does K-Means++ improve upon standard K-Means initialization?\n"
      ],
      "metadata": {
        "id": "4TnYYIlJPbgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means++ improves the initialization of centroids by selecting them in a way that spreads them out across the data space. It chooses the first centroid randomly and then selects subsequent centroids based on their distance from existing centroids, which helps to reduce the chances of poor clustering and improves convergence speed."
      ],
      "metadata": {
        "id": "QnDUWvJZYpPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What is agglomerative clustering?\n"
      ],
      "metadata": {
        "id": "OlZKIW2mPdDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agglomerative clustering is a type of hierarchical clustering that starts with each data point as its own cluster and iteratively merges the closest pairs of clusters until a single cluster is formed or a specified number of clusters is reached. It builds a hierarchy from the bottom up."
      ],
      "metadata": {
        "id": "UaMghL3AYryz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What makes Silhouette Score a better metric than just inertia for model evaluation?"
      ],
      "metadata": {
        "id": "aU8KVXwxPekW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Silhouette Score provides a measure of how well-separated the clusters are, taking into account both the cohesion (how close points in the same cluster are) and separation (how far apart clusters are). In contrast, inertia only measures the compactness of clusters without considering their separation. Therefore, the Silhouette Score can give a more comprehensive view of clustering quality, especially when clusters have different shapes or sizes."
      ],
      "metadata": {
        "id": "FjZw5EgDYuUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Questions"
      ],
      "metadata": {
        "id": "lynmEYKMOXjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot.\n"
      ],
      "metadata": {
        "id": "eX9aC89EOczN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)\n",
        "plt.title('K-Means Clustering with 4 Centers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1AOsS0xKYxxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.\n"
      ],
      "metadata": {
        "id": "R0qpEdFmR3Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
        "predicted_labels = agg_clustering.fit_predict(X_iris)\n",
        "\n",
        "# Display the first 10 predicted labels\n",
        "print(predicted_labels[:10])"
      ],
      "metadata": {
        "id": "BOOXgGV7Yz2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot.\n"
      ],
      "metadata": {
        "id": "kYgWTf7-R5Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data\n",
        "X_moons, _ = make_moons(n_samples=300, noise=0.1)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X_moons)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_dbscan, cmap='plasma')\n",
        "plt.title('DBSCAN Clustering on Moons Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RDMChicSY2D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.\n"
      ],
      "metadata": {
        "id": "3QJTAzRtR7au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X_wine = wine.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_wine_scaled = scaler.fit_transform(X_wine)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans_wine = KMeans(n_clusters=3)\n",
        "y_wine_kmeans = kmeans_wine.fit_predict(X_wine_scaled)\n",
        "\n",
        "# Print the size of each cluster\n",
        "unique, counts = np.unique(y_wine_kmeans, return_counts=True)\n",
        "cluster_sizes = dict(zip(unique, counts))\n",
        "print(cluster_sizes)"
      ],
      "metadata": {
        "id": "eAGhgH3WY59e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result.\n"
      ],
      "metadata": {
        "id": "QQ2XjxmyR9s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Generate synthetic data\n",
        "X_circles, _ = make_circles(n_samples=300, noise=0.05)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan_circles = DBSCAN(eps=0.1, min_samples=5)\n",
        "y_circles_dbscan = dbscan_circles.fit_predict(X_circles)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles_dbscan, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering on Circular Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rMEqLxvLY65A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids.\n"
      ],
      "metadata": {
        "id": "JoREXoreSBM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "\n",
        "# Apply MinMaxScaler\n",
        "scaler_cancer = MinMaxScaler()\n",
        "X_cancer_scaled = scaler_cancer.fit_transform(X_cancer)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans_cancer = KMeans(n_clusters=2)\n",
        "kmeans_cancer.fit(X_cancer_scaled)\n",
        "\n",
        "# Output the cluster centroids\n",
        "print (kmeans_cancer.cluster_centers_)"
      ],
      "metadata": {
        "id": "yFpC2_raY9ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN.\n"
      ],
      "metadata": {
        "id": "dOP9zAXwSDdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with varying standard deviations\n",
        "X_varied, _ = make_blobs(n_samples=300, centers=3, cluster_std=[0.5, 1.0, 1.5], random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan_varied = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_varied_dbscan = dbscan_varied.fit_predict(X_varied)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_varied_dbscan, cmap='plasma')\n",
        "plt.title('DBSCAN Clustering on Varied Standard Deviations')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IXF47Ny2Y_zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means.\n"
      ],
      "metadata": {
        "id": "GCXCVA-nSFyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X_digits = digits.data\n",
        "\n",
        "# Reduce dimensions using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_digits_pca = pca.fit_transform(X_digits)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans_digits = KMeans(n_clusters=10)\n",
        "y_digits_kmeans = kmeans_digits.fit_predict(X_digits_pca)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X_digits_pca[:, 0], X_digits_pca[:, 1], c=y_digits_kmeans, cmap='viridis')\n",
        "plt.title('K-Means Clustering on Digits Dataset (PCA Reduced)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o9VuTWn1ZBtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart.\n"
      ],
      "metadata": {
        "id": "PeTejD2kSH4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X_blob, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Evaluate silhouette scores for k = 2 to 5\n",
        "silhouette_scores = []\n",
        "k_values = range(2, 6)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k)\n",
        "    y_kmeans = kmeans.fit_predict(X_blob)\n",
        "    score = silhouette_score(X_blob, y_kmeans)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot the silhouette scores\n",
        "plt.bar(k_values, silhouette_scores)\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Scores for Different k Values')\n",
        "plt.xticks(k_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hGFHhMnoZDxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage\n"
      ],
      "metadata": {
        "id": "kU6O9QhsSJ2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "linked = linkage(X_iris, method='average')\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked, orientation='top', labels=iris.target_names[iris.target], distance_sort='descending', show_leaf_counts=True)\n",
        "plt.title('Dendrogram for Iris Dataset (Average Linkage)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mmqo13XyZGTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries.\n"
      ],
      "metadata": {
        "id": "Zq9p8qRUSMQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with overlapping clusters\n",
        "X_overlap, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans_overlap = KMeans(n_clusters=3)\n",
        "y_overlap_kmeans = kmeans_overlap.fit_predict(X_overlap)\n",
        "\n",
        "# Create a mesh grid for decision boundaries\n",
        "x_min, x_max = X_overlap[:, 0].min() - 1, X_overlap[:, 0].max() + 1\n",
        "y_min, y_max = X_overlap[:, 1].min() - 1, X_overlap[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predict cluster for each point in the mesh grid\n",
        "Z = kmeans_overlap.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Visualize the clusters and decision boundaries\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "plt.scatter(X_overlap[:, 0], X_overlap[:, 1], c=y_overlap_kmeans, s=50, cmap='viridis')\n",
        "plt.title('K-Means Clustering with Overlapping Clusters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QMXceDwpZJEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.\n"
      ],
      "metadata": {
        "id": "8rAFgLuhSOIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Reduce dimensions to 2D using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=5, min_samples=5)\n",
        "clusters = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=clusters, palette='tab10', legend='full', s=60)\n",
        "plt.title(\"DBSCAN Clusters on Digits Data after t-SNE\")\n",
        "plt.xlabel(\"t-SNE Component 1\")\n",
        "plt.ylabel(\"t-SNE Component 2\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "38lMDefWZLZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result.\n"
      ],
      "metadata": {
        "id": "gevZZ3TtSQTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Apply Agglomerative Clustering with complete linkage\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
        "y_agg = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
        "plt.title('Agglomerative Clustering with Complete Linkage')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hvYQPuPxaSg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot.\n"
      ],
      "metadata": {
        "id": "veQr8-brSSNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "\n",
        "# Store inertia values\n",
        "inertia_values = []\n",
        "k_values = range(2, 7)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans_cancer = KMeans(n_clusters=k)\n",
        "    kmeans_cancer.fit(X_cancer)\n",
        "    inertia_values.append(kmeans_cancer.inertia_)\n",
        "\n",
        "# Plot the inertia values\n",
        "plt.plot(k_values, inertia_values, marker='o')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Inertia Values for K-Means Clustering on Breast Cancer Dataset')\n",
        "plt.xticks(k_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6R5uNsZNZP9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage.\n"
      ],
      "metadata": {
        "id": "EuiLUgZzSUfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic concentric circles\n",
        "X_circles, _ = make_circles(n_samples=300, noise=0.05)\n",
        "\n",
        "# Apply Agglomerative Clustering with single linkage\n",
        "agg_clustering_single = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "y_single = agg_clustering_single.fit_predict(X_circles)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_single, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering with Single Linkage on Concentric Circles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ShjtLLIkZRN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise).\n"
      ],
      "metadata": {
        "id": "mIPPAr8nSWha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X_wine = wine.data\n",
        "\n",
        "# Scale the data\n",
        "scaler_wine = StandardScaler()\n",
        "X_wine_scaled = scaler_wine.fit_transform(X_wine)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan_wine = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_wine_dbscan = dbscan_wine.fit_predict(X_wine_scaled)\n",
        "\n",
        "# Count the number of clusters excluding noise\n",
        "n_clusters = len(set(y_wine_dbscan)) - (1 if -1 in y_wine_dbscan else 0)\n",
        "print(f'Number of clusters (excluding noise): {n_clusters}')"
      ],
      "metadata": {
        "id": "Tf4QywIlZUHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points.\n"
      ],
      "metadata": {
        "id": "kWqV_TD5SYZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "X_blob, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans_blob = KMeans(n_clusters=3)\n",
        "y_blob_kmeans = kmeans_blob.fit_predict(X_blob)\n",
        "\n",
        "# Visualize the clusters and cluster centers\n",
        "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob_kmeans, s=50, cmap='viridis')\n",
        "centers_blob = kmeans_blob.cluster_centers_\n",
        "plt.scatter(centers_blob[:, 0], centers_blob[:, 1], c='red', s=200, alpha=0.75)\n",
        "plt.title('KMeans Clustering with Cluster Centers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CBGx52peZWxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n"
      ],
      "metadata": {
        "id": "ieFcQ6ClSaMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan_iris = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_iris_dbscan = dbscan_iris.fit_predict(X_iris)\n",
        "\n",
        "# Count how many samples were identified as noise\n",
        "n_noise_samples = np.sum(y_iris_dbscan == -1)\n",
        "print(f'Number of samples identified as noise: {n_noise_samples}')"
      ],
      "metadata": {
        "id": "UPQW1vEjZZvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n"
      ],
      "metadata": {
        "id": "eS41zpeHSb_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic non-linearly separable data\n",
        "X_moons, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans_moons = KMeans(n_clusters=2, random_state=42)\n",
        "y_moons_kmeans = kmeans_moons.fit_predict(X_moons)\n",
        "\n",
        "# Visualize the clustering result\n",
        "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons_kmeans, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
        "plt.title('K-Means Clustering on Non-Linearly Separable Data (Moons)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Bp34XGOawyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.\n"
      ],
      "metadata": {
        "id": "qc8bH9P5Sdkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X_digits = digits.data\n",
        "\n",
        "# Reduce dimensions using PCA to 3 components\n",
        "pca_3d = PCA(n_components=3)\n",
        "X_digits_pca_3d = pca_3d.fit_transform(X_digits)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans_digits_3d = KMeans(n_clusters=10)\n",
        "y_digits_kmeans_3d = kmeans_digits_3d.fit_predict(X_digits_pca_3d)\n",
        "\n",
        "# Visualize the clusters in 3D\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_digits_pca_3d[:, 0], X_digits_pca_3d[:, 1], X_digits_pca_3d[:, 2], c=y_digits_kmeans_3d, cmap='viridis')\n",
        "ax.set_title('K-Means Clustering on Digits Dataset (3D PCA Reduced)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FXoizWSIZcIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering.\n"
      ],
      "metadata": {
        "id": "vf-TKGmxSfUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic blobs with 5 centers\n",
        "X_blobs_5, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans_blobs_5 = KMeans(n_clusters=5)\n",
        "y_blobs_kmeans_5 = kmeans_blobs_5.fit_predict(X_blobs_5)\n",
        "\n",
        "# Evaluate silhouette score\n",
        "silhouette_avg = silhouette_score(X_blobs_5, y_blobs_kmeans_5)\n",
        "print(f'Silhouette Score for KMeans with 5 centers: {silhouette_avg}')"
      ],
      "metadata": {
        "id": "O1XKD78qZdR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D.\n"
      ],
      "metadata": {
        "id": "0YD6Xa_uSg5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "\n",
        "# Reduce dimensions using PCA\n",
        "pca_cancer = PCA(n_components=2)\n",
        "X_cancer_pca = pca_cancer.fit_transform(X_cancer)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clustering_cancer = AgglomerativeClustering(n_clusters=2)\n",
        "y_cancer_agg = agg_clustering_cancer.fit_predict(X_cancer_pca)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_cancer_pca[:, 0], X_cancer_pca[:, 1], c=y_cancer_agg, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering on Breast Cancer Dataset (PCA Reduced)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_GWd3gqaZgjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side.\n"
      ],
      "metadata": {
        "id": "HbazTSIrSi0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate noisy circular data\n",
        "X_circles_noisy, _ = make_circles(n_samples=300, noise=0.1)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans_circles = KMeans(n_clusters=2)\n",
        "y_circles_kmeans = kmeans_circles.fit_predict(X_circles_noisy)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan_circles_noisy = DBSCAN(eps=0.1, min_samples=5)\n",
        "y_circles_dbscan = dbscan_circles_noisy.fit_predict(X_circles_noisy)\n",
        "\n",
        "# Plot results side-by-side\n",
        "fig, axs = plt.subplots(1 , 2, figsize=(12, 6))\n",
        "\n",
        "# KMeans results\n",
        "axs[0].scatter(X_circles_noisy[:, 0], X_circles_noisy[:, 1], c=y_circles_kmeans, cmap='viridis')\n",
        "axs[0].set_title('KMeans Clustering on Noisy Circular Data')\n",
        "\n",
        "# DBSCAN results\n",
        "axs[1].scatter(X_circles_noisy[:, 0], X_circles_noisy[:, 1], c=y_circles_dbscan, cmap='plasma')\n",
        "axs[1].set_title('DBSCAN Clustering on Noisy Circular Data')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HNenbcxmZjWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering.\n"
      ],
      "metadata": {
        "id": "0lASFrwWSkW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans_iris = KMeans(n_clusters=3)\n",
        "y_iris_kmeans = kmeans_iris.fit_predict(X_iris)\n",
        "\n",
        "# Calculate Silhouette Coefficient\n",
        "silhouette_vals = silhouette_samples(X_iris, y_iris_kmeans)\n",
        "\n",
        "# Plot Silhouette Coefficient for each sample\n",
        "plt.bar(range(len(silhouette_vals)), silhouette_vals)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Silhouette Coefficient')\n",
        "plt.title('Silhouette Coefficient for Each Sample in Iris Dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dN0VvDjaZley"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters.\n"
      ],
      "metadata": {
        "id": "7z4cAY1cSl9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "X_blobs_avg, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Apply Agglomerative Clustering with average linkage\n",
        "agg_clustering_avg = AgglomerativeClustering(n_clusters=3, linkage='average')\n",
        "y_avg = agg_clustering_avg.fit_predict(X_blobs_avg)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_blobs_avg[:, 0], X_blobs_avg[:, 1], c=y_avg, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering with Average Linkage')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "frafiImpZnsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features).\n"
      ],
      "metadata": {
        "id": "uHkBxGboSnig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X_wine = wine.data[:, :4]  # First 4 features\n",
        "y_wine_kmeans = KMeans(n_clusters=3).fit_predict(X_wine)\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "wine_df = pd.DataFrame(X_wine, columns=wine.feature_names[:4])\n",
        "wine_df['Cluster'] = y_wine_kmeans\n",
        "\n",
        "# Visualize with seaborn pairplot\n",
        "sns.pairplot(wine_df, hue='Cluster', palette='viridis')\n",
        "plt.title('KMeans Clustering on Wine Dataset (First 4 Features)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YTn0mry1Zpy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count.\n"
      ],
      "metadata": {
        "id": "1dp1U_qUSpMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate noisy blobs\n",
        "X_noisy_blobs, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan_noisy = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_noisy_dbscan = dbscan_noisy.fit_predict(X_noisy_blobs)\n",
        "\n",
        "# Count clusters and noise points\n",
        "n_clusters_noisy = len(set(y_noisy_dbscan)) - (1 if -1 in y_noisy_dbscan else 0)\n",
        "n_noise_points = np.sum(y_noisy_dbscan == -1)\n",
        "\n",
        "print(f'Number of clusters (excluding noise): {n_clusters_noisy}')\n",
        "print(f'Number of noise points: {n_noise_points}')"
      ],
      "metadata": {
        "id": "OTAFrHPLZt1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters."
      ],
      "metadata": {
        "id": "BDmYKOZiSqs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X_digits = digits.data\n",
        "\n",
        "# Reduce dimensions using t-SNE\n",
        "tsne_digits = TSNE(n_components=2, random_state=42)\n",
        "X_digits_tsne = tsne_digits.fit_transform(X_digits)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clustering_digits = AgglomerativeClustering(n_clusters=10)\n",
        "y_digits_agg = agg_clustering_digits.fit_predict(X_digits_tsne)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1], c=y_digits_agg, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering on Digits Dataset (t-SNE Reduced)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MS8i6KDLZwym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}